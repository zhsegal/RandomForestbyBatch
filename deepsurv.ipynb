{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.optimizers import SGD, RMSprop\n",
    "from keras import regularizers \n",
    "import keras.backend as K\n",
    "\n",
    "from lifelines.utils import concordance_index\n",
    "from lifelines import CoxPHFitter\n",
    "\n",
    "from lifelines.datasets import load_rossi\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: pandas\n",
      "Version: 0.24.2\n",
      "Summary: Powerful data structures for data analysis, time series, and statistics\n",
      "Home-page: http://pandas.pydata.org\n",
      "Author: None\n",
      "Author-email: None\n",
      "License: BSD\n",
      "Location: /usr/local/lib/python3.7/site-packages\n",
      "Requires: numpy, pytz, python-dateutil\n",
      "Required-by: statsmodels, sklearn-pandas, shap, seaborn, scikit-survival, random-survival-forest, pysurvival, pgmpy, mlflow, lifelines\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('/Users/zvisegal/Desktop/patient2rank/data/x_train.csv')\n",
    "X_validation = pd.read_csv('/Users/zvisegal/Desktop/patient2rank/data/x_validation.csv')\n",
    "X_test = pd.read_csv('/Users/zvisegal/Desktop/patient2rank/data/x_test.csv')\n",
    "X_train = X_train.set_index('patient_id', drop=True)\n",
    "X_validation = X_validation.set_index('patient_id', drop=True)\n",
    "X_test = X_test.set_index('patient_id', drop=True)\n",
    "X=pd.concat([X_train,X_test,X_validation], ignore_index=True)\n",
    "\n",
    "y = pd.concat([X_train.target, X_validation.target, X_test.target])\n",
    "\n",
    "y.rename('target')\n",
    "\n",
    "y_survival = pd.read_csv('/Users/zvisegal/Desktop/patient2rank/data/y_survival.csv')\n",
    "y_survival = y_survival.set_index('patient_id')\n",
    "y_survival = y_survival[['target', 'time']]\n",
    "y_survival.target = y_survival.target.astype(bool)\n",
    "y_survival.time = y_survival.time.astype('double')\n",
    "\n",
    "y_survival = y_survival.to_records(index=False)\n",
    "Y=y_survival\n",
    "# global_time = '2020-03-10 15:23:35'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rec.array([(False, 692.), (False, 656.), (False, 704.), ( True, 402.),\n",
       "           (False, 690.), (False, 652.), (False, 624.), (False, 666.),\n",
       "           (False, 704.), (False, 692.), (False, 669.), (False, 698.),\n",
       "           ( True, 152.), (False, 654.), (False, 695.), ( True, 416.),\n",
       "           (False, 620.), ( True, 315.), (False, 699.), ( True, 517.),\n",
       "           (False, 693.), (False, 705.), (False, 654.), (False, 624.),\n",
       "           (False, 664.), (False, 615.), ( True, 553.), (False, 678.),\n",
       "           (False, 640.), (False, 690.), (False, 705.), (False, 687.),\n",
       "           (False, 646.), (False, 686.), (False, 629.), ( True, 181.),\n",
       "           (False, 672.), ( True, 179.), ( True, 407.), (False, 670.),\n",
       "           (False, 644.), ( True, 516.), ( True,  93.), (False, 651.),\n",
       "           (False, 698.), (False, 662.), ( True, 145.), (False, 639.),\n",
       "           (False, 684.), (False, 630.), (False, 682.), (False, 664.),\n",
       "           (False, 682.), ( True, 435.), (False, 648.), ( True, 494.),\n",
       "           (False, 691.), (False, 661.), ( True,  30.), ( True, 514.),\n",
       "           (False, 667.), (False, 679.), (False, 698.), (False, 689.),\n",
       "           (False, 657.), (False, 683.), (False, 650.), (False, 645.),\n",
       "           (False, 678.), (False, 658.), (False, 633.), ( True, 246.),\n",
       "           (False, 699.), ( True, 183.), ( True, 271.), (False, 645.),\n",
       "           (False, 644.), (False, 662.), (False, 702.), (False, 654.),\n",
       "           (False, 668.), (False, 690.), (False, 651.), (False, 685.),\n",
       "           (False, 637.), (False, 688.), (False, 638.), ( True,  32.),\n",
       "           (False, 702.), (False, 702.), (False, 661.), (False, 635.),\n",
       "           (False, 701.), (False, 661.), (False, 640.), (False, 640.),\n",
       "           (False, 622.), (False, 701.), (False, 695.), (False, 661.),\n",
       "           (False, 620.), (False, 646.), (False, 617.), (False, 649.),\n",
       "           (False, 703.), ( True, 543.), (False, 619.), (False, 684.),\n",
       "           ( True, 209.), (False, 708.), (False, 636.), (False, 674.),\n",
       "           (False, 661.), (False, 691.), (False, 676.), (False, 675.),\n",
       "           (False, 681.), ( True,  32.), ( True, 118.), (False, 675.),\n",
       "           (False, 677.), (False, 658.), (False, 669.), (False, 707.),\n",
       "           (False, 697.), (False, 647.), (False, 670.), (False, 686.),\n",
       "           (False, 664.), (False, 668.), (False, 693.), (False, 641.),\n",
       "           (False, 633.), ( True, 596.), (False, 665.), (False, 666.),\n",
       "           (False, 651.), ( True, 206.), (False, 635.), (False, 655.),\n",
       "           (False, 637.), (False, 695.), (False, 662.), (False, 659.),\n",
       "           (False, 631.), (False, 648.), (False, 651.), (False, 670.),\n",
       "           (False, 631.), (False, 643.), (False, 654.), ( True, 213.),\n",
       "           (False, 686.), ( True, 599.), (False, 620.), ( True, 502.),\n",
       "           (False, 667.), (False, 617.), (False, 679.), (False, 657.),\n",
       "           (False, 699.), (False, 618.), ( True, 257.), (False, 654.),\n",
       "           (False, 659.), (False, 652.), (False, 635.), (False, 626.),\n",
       "           (False, 655.), (False, 655.), (False, 630.), (False, 659.),\n",
       "           (False, 670.), (False, 658.), (False, 672.), (False, 697.),\n",
       "           (False, 668.), (False, 674.), (False, 682.), ( True,  95.),\n",
       "           (False, 700.), (False, 640.), (False, 624.), (False, 664.),\n",
       "           ( True, 226.), (False, 670.), (False, 655.), (False, 689.),\n",
       "           (False, 643.), (False, 658.), (False, 662.), (False, 683.),\n",
       "           (False, 627.), (False, 643.), (False, 701.), ( True, 147.),\n",
       "           (False, 684.), (False, 705.), ( True,  20.), (False, 646.),\n",
       "           ( True, 354.), (False, 675.), (False, 679.), (False, 641.),\n",
       "           (False, 656.), ( True, 454.), (False, 680.), (False, 622.),\n",
       "           (False, 682.), (False, 676.), ( True, 337.), ( True, 444.),\n",
       "           ( True, 631.), (False, 636.), (False, 694.), (False, 660.),\n",
       "           (False, 646.), (False, 685.), (False, 689.), ( True, 366.),\n",
       "           ( True, 113.), (False, 704.), (False, 640.), (False, 623.),\n",
       "           (False, 636.), (False, 619.), (False, 666.), ( True, 146.),\n",
       "           (False, 658.), (False, 695.), (False, 705.), (False, 663.),\n",
       "           ( True,  71.), (False, 653.), ( True, 260.), ( True,  33.),\n",
       "           (False, 665.), (False, 688.), (False, 627.), (False, 689.),\n",
       "           (False, 662.), (False, 702.), ( True, 173.), (False, 668.),\n",
       "           (False, 685.), (False, 665.), ( True,  43.), (False, 675.),\n",
       "           (False, 624.), (False, 671.), ( True, 145.), (False, 636.),\n",
       "           (False, 706.), (False, 708.), (False, 636.), (False, 642.),\n",
       "           (False, 667.), (False, 687.), (False, 686.), (False, 649.),\n",
       "           (False, 681.), (False, 694.), (False, 629.), (False, 654.),\n",
       "           (False, 626.), (False, 674.), (False, 687.), (False, 643.),\n",
       "           (False, 679.), (False, 643.), (False, 658.), ( True, 584.),\n",
       "           (False, 681.), (False, 671.), (False, 649.), (False, 633.),\n",
       "           (False, 662.), (False, 675.), (False, 651.), (False, 653.),\n",
       "           (False, 694.), (False, 692.), (False, 699.), (False, 628.),\n",
       "           (False, 704.), ( True, 138.), (False, 665.), (False, 679.),\n",
       "           (False, 682.), ( True, 541.), ( True, 297.), (False, 656.),\n",
       "           (False, 632.), (False, 689.), (False, 648.), (False, 705.),\n",
       "           (False, 682.), (False, 690.), (False, 659.), (False, 639.),\n",
       "           ( True,  29.), (False, 634.), ( True, 405.), ( True, 398.),\n",
       "           (False, 660.), (False, 676.), (False, 710.), (False, 710.),\n",
       "           (False, 648.), (False, 636.), (False, 701.), (False, 620.),\n",
       "           (False, 636.), (False, 679.), (False, 705.), (False, 645.),\n",
       "           (False, 700.), (False, 618.), (False, 670.), ( True, 222.),\n",
       "           (False, 692.), (False, 630.), ( True, 557.), (False, 700.),\n",
       "           ( True, 320.), (False, 631.), (False, 637.), (False, 687.),\n",
       "           (False, 709.), (False, 658.), (False, 672.), ( True, 283.),\n",
       "           ( True, 438.), (False, 652.), (False, 647.), (False, 650.),\n",
       "           (False, 659.), (False, 671.), (False, 662.), (False, 640.),\n",
       "           (False, 644.), ( True, 657.), (False, 659.), ( True,  46.),\n",
       "           (False, 702.), (False, 671.), (False, 674.), ( True, 219.),\n",
       "           (False, 707.), (False, 648.), (False, 703.), (False, 672.),\n",
       "           (False, 660.), ( True, 182.), (False, 705.), (False, 669.),\n",
       "           (False, 690.), ( True,  67.), (False, 684.), (False, 680.),\n",
       "           (False, 627.), (False, 685.), (False, 627.), (False, 673.),\n",
       "           ( True, 320.), ( True, 334.), (False, 635.), (False, 691.),\n",
       "           (False, 646.), (False, 658.), ( True, 575.), ( True, 117.),\n",
       "           (False, 671.), (False, 700.), (False, 684.), (False, 637.),\n",
       "           (False, 696.), (False, 696.), (False, 655.), (False, 701.),\n",
       "           ( True, 517.), (False, 619.), (False, 624.), (False, 641.),\n",
       "           (False, 656.), (False, 696.), (False, 642.), (False, 689.),\n",
       "           ( True, 409.), (False, 615.), (False, 668.), (False, 677.),\n",
       "           (False, 676.), (False, 646.), ( True, 252.), (False, 675.),\n",
       "           (False, 647.), (False, 679.), (False, 640.), (False, 625.)],\n",
       "          dtype=[('target', '?'), ('time', '<f8')])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_survival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "E=y_survival.target.astype(int)\n",
    "Y=y_survival.time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X.drop([ 'trial_start_time',\n",
    "       'max_time', 'eligability_end_date', 'inclusion', 'exclusion',\n",
    "       'target', 'target_diagnosis_time', 'target_code', 'target_name',\n",
    "       'target_type', 'exclusion_diagnosis_time', 'exclusion_code',\n",
    "       'inclusion_diagnosis_time', 'inclusion_code', 'duration_pre_diag'],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_val,Y_train,Y_val=train_test_split(X,Y,test_size=0.25, random_state=0)\n",
    "X_train,X_val,E_train,E_val=train_test_split(X,E,test_size=0.25, random_state=0)\n",
    "#Standardize\n",
    "scaler=preprocessing.StandardScaler().fit(X_train)\n",
    "X_train=scaler.transform(X_train)\n",
    "# X_val=scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.95437311,  1.03035687, -0.77735325, ..., -0.08732634,\n",
       "        -0.28743396, -0.13177786],\n",
       "       [ 1.04780824,  0.27902793,  1.28641644, ...,  0.48416164,\n",
       "         1.45452668,  0.41265625],\n",
       "       [-0.95437311, -0.09663653,  1.28641644, ..., -0.08732634,\n",
       "        -0.28743396, -0.13177786],\n",
       "       ...,\n",
       "       [-0.95437311, -0.75411364, -0.77735325, ..., -0.08732634,\n",
       "        -0.28743396, -0.13177786],\n",
       "       [ 1.04780824, -0.37844917, -0.77735325, ..., -0.08732634,\n",
       "        -0.28743396, -0.13177786],\n",
       "       [ 1.04780824,  0.6546924 , -0.77735325, ..., -0.08732634,\n",
       "        -0.28743396, -0.13177786]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Sorting for NNL!\n",
    "sort_idx = np.argsort(Y_train)[::-1]\n",
    "X_train=X_train[sort_idx]\n",
    "Y_train=Y_train[sort_idx]\n",
    "E_train=E_train[sort_idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "input_shape=X_train.shape[1]\n",
    "scaler=preprocessing.StandardScaler()\n",
    "minmax_scaler = MinMaxScaler()\n",
    "pipeline = Pipeline([('scaler', scaler)])\n",
    "pipeline.steps.append(('normalize', minmax_scaler))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('scaler',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "                ('normalize', MinMaxScaler(copy=True, feature_range=(0, 1)))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CALLBACKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import callbacks\n",
    "import time\n",
    "\n",
    "name=f'deepsurv_opioid{int(time.time())}'\n",
    "tensoboard=callbacks.TensorBoard(\n",
    "    log_dir=f'deep_surv_logs/{name}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import datetime, os\n",
    "\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=deep_surv_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:13: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(64, input_shape=(443,), kernel_initializer=\"glorot_uniform\")`\n",
      "  del sys.path[0]\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(64, kernel_initializer=\"glorot_uniform\")`\n",
      "  from ipykernel import kernelapp as app\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:19: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, activation=\"linear\", activity_regularizer=<keras.reg..., kernel_initializer=\"glorot_uniform\", kernel_regularizer=<keras.reg...)`\n"
     ]
    }
   ],
   "source": [
    "def negative_log_likelihood(E):\n",
    "    def loss(y_true, y_pred):\n",
    "        hazard_ratio = K.exp(y_pred)\n",
    "        log_risk = K.log(K.cumsum(hazard_ratio, axis=1))\n",
    "        uncensored_likelihood = y_pred - log_risk\n",
    "        censored_likelihood = uncensored_likelihood * E\n",
    "        neg_likelihood = -K.sum(censored_likelihood)\n",
    "        return neg_likelihood\n",
    "    return loss\n",
    "\n",
    "#Keras model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_shape=(input_shape,), init='glorot_uniform')) # shape= length, dimension\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(64, init='glorot_uniform'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.7))\n",
    "model.add(Dense(1,activation=\"linear\", init='glorot_uniform', \n",
    "                W_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l2(0.01)))\n",
    "#\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=0.01, momentum=0.9, nesterov=True)\n",
    "rmsprop=RMSprop(lr=1e-5, rho=0.9, epsilon=1e-8)\n",
    "model.name=\"deep_survival\"\n",
    "\n",
    "model.compile(loss=negative_log_likelihood(E_train), optimizer=rmsprop)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    " pipeline.steps.append(['model', model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('scaler',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "                ('normalize', MinMaxScaler(copy=True, feature_range=(0, 1))),\n",
       "                ['model',\n",
       "                 <keras.engine.sequential.Sequential object at 0x14bd37090>]],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL FIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Train on 200 samples, validate on 100 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:2: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n",
      "200/200 [==============================] - 0s 22us/step - loss: 0.4158 - val_loss: 0.0681\n",
      "Epoch 2/400\n",
      "200/200 [==============================] - 0s 21us/step - loss: 0.4244 - val_loss: 0.0682\n",
      "Epoch 3/400\n",
      "200/200 [==============================] - 0s 25us/step - loss: 0.3314 - val_loss: 0.0683\n",
      "Epoch 4/400\n",
      "200/200 [==============================] - 0s 25us/step - loss: 0.3353 - val_loss: 0.0683\n",
      "Epoch 5/400\n",
      "200/200 [==============================] - 0s 25us/step - loss: 0.4096 - val_loss: 0.0681\n",
      "Epoch 6/400\n",
      "200/200 [==============================] - 0s 24us/step - loss: 0.5759 - val_loss: 0.0681\n",
      "Epoch 7/400\n",
      "200/200 [==============================] - 0s 23us/step - loss: 0.5878 - val_loss: 0.0681\n",
      "Epoch 8/400\n",
      "200/200 [==============================] - 0s 25us/step - loss: 0.5900 - val_loss: 0.0681\n",
      "Epoch 9/400\n",
      "200/200 [==============================] - 0s 23us/step - loss: 0.4357 - val_loss: 0.0680\n",
      "Epoch 10/400\n",
      "200/200 [==============================] - 0s 23us/step - loss: 0.4283 - val_loss: 0.0680\n",
      "Epoch 11/400\n",
      "200/200 [==============================] - 0s 21us/step - loss: 0.3475 - val_loss: 0.0680\n",
      "Epoch 12/400\n",
      "200/200 [==============================] - 0s 20us/step - loss: 0.4628 - val_loss: 0.0681\n",
      "Epoch 13/400\n",
      "200/200 [==============================] - 0s 26us/step - loss: 0.3382 - val_loss: 0.0681\n",
      "Epoch 14/400\n",
      "200/200 [==============================] - 0s 26us/step - loss: 0.2464 - val_loss: 0.0683\n",
      "Epoch 15/400\n",
      "200/200 [==============================] - 0s 24us/step - loss: 0.4988 - val_loss: 0.0683\n",
      "Epoch 16/400\n",
      "200/200 [==============================] - 0s 24us/step - loss: 0.5634 - val_loss: 0.0683\n",
      "Epoch 17/400\n",
      "200/200 [==============================] - 0s 22us/step - loss: 0.4938 - val_loss: 0.0682\n",
      "Epoch 18/400\n",
      "200/200 [==============================] - 0s 26us/step - loss: 0.5458 - val_loss: 0.0680\n",
      "Epoch 19/400\n",
      "200/200 [==============================] - 0s 19us/step - loss: 0.5514 - val_loss: 0.0678\n",
      "Epoch 20/400\n",
      "200/200 [==============================] - 0s 24us/step - loss: 0.6783 - val_loss: 0.0677\n",
      "Epoch 21/400\n",
      "200/200 [==============================] - 0s 22us/step - loss: 0.5582 - val_loss: 0.0677\n",
      "Epoch 22/400\n",
      "200/200 [==============================] - 0s 24us/step - loss: 0.6493 - val_loss: 0.0678\n",
      "Epoch 23/400\n",
      "200/200 [==============================] - 0s 28us/step - loss: 0.5938 - val_loss: 0.0676\n",
      "Epoch 24/400\n",
      "200/200 [==============================] - 0s 26us/step - loss: 0.5464 - val_loss: 0.0675\n",
      "Epoch 25/400\n",
      "200/200 [==============================] - 0s 23us/step - loss: 0.6834 - val_loss: 0.0675\n",
      "Epoch 26/400\n",
      "200/200 [==============================] - 0s 25us/step - loss: 0.3892 - val_loss: 0.0676\n",
      "Epoch 27/400\n",
      "200/200 [==============================] - 0s 21us/step - loss: 0.4424 - val_loss: 0.0676\n",
      "Epoch 28/400\n",
      "200/200 [==============================] - 0s 25us/step - loss: 0.3074 - val_loss: 0.0677\n",
      "Epoch 29/400\n",
      "200/200 [==============================] - 0s 33us/step - loss: 0.4336 - val_loss: 0.0676\n",
      "Epoch 30/400\n",
      "200/200 [==============================] - 0s 26us/step - loss: 0.3351 - val_loss: 0.0677\n",
      "Epoch 31/400\n",
      "200/200 [==============================] - 0s 34us/step - loss: 0.4954 - val_loss: 0.0678\n",
      "Epoch 32/400\n",
      "200/200 [==============================] - 0s 20us/step - loss: 0.3370 - val_loss: 0.0677\n",
      "Epoch 33/400\n",
      "200/200 [==============================] - 0s 21us/step - loss: 0.2954 - val_loss: 0.0677\n",
      "Epoch 34/400\n",
      "200/200 [==============================] - 0s 24us/step - loss: 0.6791 - val_loss: 0.0676\n",
      "Epoch 35/400\n",
      "200/200 [==============================] - 0s 21us/step - loss: 0.4031 - val_loss: 0.0677\n",
      "Epoch 36/400\n",
      "200/200 [==============================] - 0s 27us/step - loss: 0.3725 - val_loss: 0.0676\n",
      "Epoch 37/400\n",
      "200/200 [==============================] - 0s 20us/step - loss: 0.3869 - val_loss: 0.0675\n",
      "Epoch 38/400\n",
      "200/200 [==============================] - 0s 21us/step - loss: 0.4073 - val_loss: 0.0677\n",
      "Epoch 39/400\n",
      "200/200 [==============================] - 0s 27us/step - loss: 0.2678 - val_loss: 0.0675\n",
      "Epoch 40/400\n",
      "200/200 [==============================] - 0s 26us/step - loss: 0.4442 - val_loss: 0.0675\n",
      "Epoch 41/400\n",
      "200/200 [==============================] - 0s 30us/step - loss: 0.8145 - val_loss: 0.0673\n",
      "Epoch 42/400\n",
      "200/200 [==============================] - 0s 25us/step - loss: 0.3467 - val_loss: 0.0672\n",
      "Epoch 43/400\n",
      "200/200 [==============================] - 0s 25us/step - loss: 0.4056 - val_loss: 0.0672\n",
      "Epoch 44/400\n",
      "200/200 [==============================] - 0s 26us/step - loss: 0.5785 - val_loss: 0.0673\n",
      "Epoch 45/400\n",
      "200/200 [==============================] - 0s 24us/step - loss: 0.3749 - val_loss: 0.0673\n",
      "Epoch 46/400\n",
      "200/200 [==============================] - 0s 27us/step - loss: 0.4419 - val_loss: 0.0673\n",
      "Epoch 47/400\n",
      "200/200 [==============================] - 0s 21us/step - loss: 0.9772 - val_loss: 0.0672\n",
      "Epoch 48/400\n",
      "200/200 [==============================] - 0s 26us/step - loss: 0.3739 - val_loss: 0.0672\n",
      "Epoch 49/400\n",
      "200/200 [==============================] - 0s 24us/step - loss: 0.4770 - val_loss: 0.0673\n",
      "Epoch 50/400\n",
      "200/200 [==============================] - 0s 27us/step - loss: 0.3760 - val_loss: 0.0672\n",
      "Epoch 51/400\n",
      "200/200 [==============================] - 0s 27us/step - loss: 0.8266 - val_loss: 0.0671\n",
      "Epoch 52/400\n",
      "200/200 [==============================] - 0s 22us/step - loss: 0.4954 - val_loss: 0.0671\n",
      "Epoch 53/400\n",
      "200/200 [==============================] - 0s 24us/step - loss: 0.8437 - val_loss: 0.0672\n",
      "Epoch 54/400\n",
      "200/200 [==============================] - 0s 27us/step - loss: 0.4917 - val_loss: 0.0672\n",
      "Epoch 55/400\n",
      "200/200 [==============================] - 0s 35us/step - loss: 0.5565 - val_loss: 0.0671\n",
      "Epoch 56/400\n",
      "200/200 [==============================] - 0s 38us/step - loss: 0.4715 - val_loss: 0.0671\n",
      "Epoch 57/400\n",
      "200/200 [==============================] - 0s 27us/step - loss: 0.4814 - val_loss: 0.0671\n",
      "Epoch 58/400\n",
      "200/200 [==============================] - 0s 25us/step - loss: 0.2859 - val_loss: 0.0670\n",
      "Epoch 59/400\n",
      "200/200 [==============================] - 0s 23us/step - loss: 0.4162 - val_loss: 0.0670\n",
      "Epoch 60/400\n",
      "200/200 [==============================] - 0s 28us/step - loss: 0.5696 - val_loss: 0.0669\n",
      "Epoch 61/400\n",
      "200/200 [==============================] - 0s 29us/step - loss: 0.3443 - val_loss: 0.0669\n",
      "Epoch 62/400\n",
      "200/200 [==============================] - 0s 29us/step - loss: 0.4213 - val_loss: 0.0668\n",
      "Epoch 63/400\n",
      "200/200 [==============================] - 0s 24us/step - loss: 0.5116 - val_loss: 0.0668\n",
      "Epoch 64/400\n",
      "200/200 [==============================] - 0s 31us/step - loss: 0.2351 - val_loss: 0.0667\n",
      "Epoch 65/400\n",
      "200/200 [==============================] - 0s 20us/step - loss: 0.3866 - val_loss: 0.0667\n",
      "Epoch 66/400\n",
      "200/200 [==============================] - 0s 25us/step - loss: 0.3452 - val_loss: 0.0667\n",
      "Epoch 67/400\n",
      "200/200 [==============================] - 0s 31us/step - loss: 0.4425 - val_loss: 0.0666\n",
      "Epoch 68/400\n",
      "200/200 [==============================] - 0s 29us/step - loss: 0.9623 - val_loss: 0.0666\n",
      "Epoch 69/400\n",
      "200/200 [==============================] - 0s 24us/step - loss: 0.3255 - val_loss: 0.0666\n",
      "Epoch 70/400\n",
      "200/200 [==============================] - 0s 24us/step - loss: 0.4069 - val_loss: 0.0666\n",
      "Epoch 71/400\n",
      "200/200 [==============================] - 0s 24us/step - loss: 0.4283 - val_loss: 0.0666\n",
      "Epoch 72/400\n",
      "200/200 [==============================] - 0s 22us/step - loss: 0.3821 - val_loss: 0.0666\n",
      "Epoch 73/400\n",
      "200/200 [==============================] - 0s 21us/step - loss: 0.4842 - val_loss: 0.0665\n",
      "Epoch 74/400\n",
      "200/200 [==============================] - 0s 24us/step - loss: 0.4797 - val_loss: 0.0665\n",
      "Epoch 75/400\n",
      "200/200 [==============================] - 0s 23us/step - loss: 0.4983 - val_loss: 0.0666\n",
      "Epoch 76/400\n",
      "200/200 [==============================] - 0s 21us/step - loss: 0.5624 - val_loss: 0.0665\n",
      "Epoch 77/400\n",
      "200/200 [==============================] - 0s 22us/step - loss: 0.4757 - val_loss: 0.0665\n",
      "Epoch 78/400\n",
      "200/200 [==============================] - 0s 24us/step - loss: 0.5343 - val_loss: 0.0664\n",
      "Epoch 79/400\n",
      "200/200 [==============================] - 0s 21us/step - loss: 0.4591 - val_loss: 0.0663\n",
      "Epoch 80/400\n",
      "200/200 [==============================] - 0s 20us/step - loss: 0.5242 - val_loss: 0.0662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/400\n",
      "200/200 [==============================] - 0s 20us/step - loss: 0.4688 - val_loss: 0.0663\n",
      "Epoch 82/400\n",
      "200/200 [==============================] - 0s 22us/step - loss: 0.5302 - val_loss: 0.0662\n",
      "Epoch 83/400\n",
      "200/200 [==============================] - 0s 23us/step - loss: 0.3212 - val_loss: 0.0663\n",
      "Epoch 84/400\n",
      "200/200 [==============================] - 0s 21us/step - loss: 0.4138 - val_loss: 0.0662\n",
      "Epoch 85/400\n",
      "200/200 [==============================] - 0s 23us/step - loss: 0.4140 - val_loss: 0.0661\n",
      "Epoch 86/400\n",
      "200/200 [==============================] - 0s 22us/step - loss: 0.2882 - val_loss: 0.0662\n",
      "Epoch 87/400\n",
      "200/200 [==============================] - 0s 24us/step - loss: 0.5855 - val_loss: 0.0661\n",
      "Epoch 88/400\n",
      "200/200 [==============================] - 0s 21us/step - loss: 0.4782 - val_loss: 0.0661\n",
      "Epoch 89/400\n",
      "200/200 [==============================] - 0s 22us/step - loss: 0.3080 - val_loss: 0.0661\n",
      "Epoch 90/400\n",
      "200/200 [==============================] - 0s 27us/step - loss: 0.2330 - val_loss: 0.0662\n",
      "Epoch 91/400\n",
      "200/200 [==============================] - 0s 20us/step - loss: 0.2786 - val_loss: 0.0661\n",
      "Epoch 92/400\n",
      "200/200 [==============================] - 0s 23us/step - loss: 0.4098 - val_loss: 0.0660\n",
      "Epoch 93/400\n",
      "200/200 [==============================] - 0s 22us/step - loss: 0.5896 - val_loss: 0.0661\n",
      "Epoch 94/400\n",
      "200/200 [==============================] - 0s 19us/step - loss: 0.2569 - val_loss: 0.0661\n",
      "Epoch 95/400\n",
      "200/200 [==============================] - 0s 23us/step - loss: 0.3469 - val_loss: 0.0662\n",
      "Epoch 96/400\n",
      "200/200 [==============================] - 0s 20us/step - loss: 0.8787 - val_loss: 0.0661\n",
      "Epoch 97/400\n",
      "200/200 [==============================] - 0s 23us/step - loss: 0.4952 - val_loss: 0.0661\n",
      "Epoch 98/400\n",
      "200/200 [==============================] - 0s 25us/step - loss: 0.5323 - val_loss: 0.0660\n",
      "Epoch 99/400\n",
      "200/200 [==============================] - 0s 22us/step - loss: 0.4190 - val_loss: 0.0660\n",
      "Epoch 100/400\n",
      "200/200 [==============================] - 0s 26us/step - loss: 0.3552 - val_loss: 0.0659\n",
      "Epoch 101/400\n",
      "200/200 [==============================] - 0s 25us/step - loss: 0.3617 - val_loss: 0.0658\n",
      "Epoch 102/400\n",
      "200/200 [==============================] - 0s 20us/step - loss: 0.3734 - val_loss: 0.0659\n",
      "Epoch 103/400\n",
      "200/200 [==============================] - 0s 23us/step - loss: 0.5469 - val_loss: 0.0659\n",
      "Epoch 104/400\n",
      "200/200 [==============================] - 0s 21us/step - loss: 0.6064 - val_loss: 0.0661\n",
      "Epoch 105/400\n",
      "200/200 [==============================] - 0s 22us/step - loss: 0.4265 - val_loss: 0.0660\n",
      "Epoch 106/400\n",
      "200/200 [==============================] - 0s 23us/step - loss: 0.2125 - val_loss: 0.0659\n",
      "Epoch 107/400\n",
      "200/200 [==============================] - 0s 21us/step - loss: 0.4254 - val_loss: 0.0659\n",
      "Epoch 108/400\n",
      "200/200 [==============================] - 0s 23us/step - loss: 0.3655 - val_loss: 0.0660\n",
      "Epoch 109/400\n",
      "200/200 [==============================] - 0s 25us/step - loss: 0.5248 - val_loss: 0.0659\n",
      "Epoch 110/400\n",
      "200/200 [==============================] - 0s 21us/step - loss: 0.3521 - val_loss: 0.0659\n",
      "Epoch 111/400\n",
      "200/200 [==============================] - 0s 23us/step - loss: 0.4357 - val_loss: 0.0659\n",
      "Epoch 112/400\n",
      "200/200 [==============================] - 0s 27us/step - loss: 0.3715 - val_loss: 0.0659\n",
      "Epoch 113/400\n",
      "200/200 [==============================] - 0s 22us/step - loss: 0.3406 - val_loss: 0.0659\n",
      "Epoch 114/400\n",
      "200/200 [==============================] - 0s 22us/step - loss: 0.3073 - val_loss: 0.0658\n",
      "Epoch 115/400\n",
      "200/200 [==============================] - 0s 23us/step - loss: 0.5000 - val_loss: 0.0657\n",
      "Epoch 116/400\n",
      "200/200 [==============================] - 0s 22us/step - loss: 0.3054 - val_loss: 0.0657\n",
      "Epoch 117/400\n",
      "200/200 [==============================] - 0s 24us/step - loss: 0.3546 - val_loss: 0.0658\n",
      "Epoch 118/400\n",
      "200/200 [==============================] - 0s 25us/step - loss: 0.5406 - val_loss: 0.0657\n",
      "Epoch 119/400\n",
      "200/200 [==============================] - 0s 23us/step - loss: 0.4267 - val_loss: 0.0656\n",
      "Epoch 120/400\n",
      "200/200 [==============================] - 0s 28us/step - loss: 0.4175 - val_loss: 0.0655\n",
      "Epoch 121/400\n",
      "200/200 [==============================] - 0s 21us/step - loss: 0.3679 - val_loss: 0.0654\n",
      "Epoch 122/400\n",
      "200/200 [==============================] - 0s 21us/step - loss: 0.4730 - val_loss: 0.0654\n",
      "Epoch 123/400\n",
      "200/200 [==============================] - 0s 33us/step - loss: 0.3899 - val_loss: 0.0653\n",
      "Epoch 124/400\n",
      "200/200 [==============================] - 0s 26us/step - loss: 0.5486 - val_loss: 0.0654\n",
      "Epoch 125/400\n",
      "200/200 [==============================] - 0s 31us/step - loss: 0.3115 - val_loss: 0.0655\n",
      "Epoch 126/400\n",
      "200/200 [==============================] - 0s 23us/step - loss: 0.3477 - val_loss: 0.0654\n",
      "Epoch 127/400\n",
      "200/200 [==============================] - 0s 28us/step - loss: 0.5107 - val_loss: 0.0654\n",
      "Epoch 128/400\n",
      "200/200 [==============================] - 0s 25us/step - loss: 0.3376 - val_loss: 0.0654\n",
      "Epoch 129/400\n",
      "200/200 [==============================] - 0s 22us/step - loss: 0.3027 - val_loss: 0.0654\n",
      "Epoch 130/400\n",
      "200/200 [==============================] - 0s 26us/step - loss: 0.3438 - val_loss: 0.0654\n",
      "Epoch 131/400\n",
      "200/200 [==============================] - 0s 20us/step - loss: 0.5232 - val_loss: 0.0653\n",
      "Epoch 132/400\n",
      "200/200 [==============================] - 0s 24us/step - loss: 0.5937 - val_loss: 0.0652\n",
      "Epoch 133/400\n",
      "200/200 [==============================] - 0s 25us/step - loss: 0.4055 - val_loss: 0.0651\n",
      "Epoch 134/400\n",
      "200/200 [==============================] - 0s 22us/step - loss: 0.2595 - val_loss: 0.0651\n",
      "Epoch 135/400\n",
      "200/200 [==============================] - 0s 19us/step - loss: 0.3176 - val_loss: 0.0651\n",
      "Epoch 136/400\n",
      "200/200 [==============================] - 0s 25us/step - loss: 0.3951 - val_loss: 0.0652\n",
      "Epoch 137/400\n",
      "200/200 [==============================] - 0s 21us/step - loss: 0.3736 - val_loss: 0.0651\n",
      "Epoch 138/400\n",
      "200/200 [==============================] - 0s 23us/step - loss: 0.4608 - val_loss: 0.0650\n",
      "Epoch 139/400\n",
      "200/200 [==============================] - 0s 24us/step - loss: 0.5145 - val_loss: 0.0650\n",
      "Epoch 140/400\n",
      "200/200 [==============================] - 0s 23us/step - loss: 0.3506 - val_loss: 0.0649\n",
      "Epoch 141/400\n",
      "200/200 [==============================] - 0s 28us/step - loss: 0.4606 - val_loss: 0.0649\n",
      "Epoch 142/400\n",
      "200/200 [==============================] - 0s 24us/step - loss: 0.3701 - val_loss: 0.0648\n",
      "Epoch 143/400\n",
      "200/200 [==============================] - 0s 22us/step - loss: 0.3881 - val_loss: 0.0649\n",
      "Epoch 144/400\n",
      "200/200 [==============================] - 0s 26us/step - loss: 0.4372 - val_loss: 0.0650\n",
      "Epoch 145/400\n",
      "200/200 [==============================] - 0s 31us/step - loss: 0.3099 - val_loss: 0.0650\n",
      "Epoch 146/400\n",
      "200/200 [==============================] - 0s 29us/step - loss: 0.3612 - val_loss: 0.0649\n",
      "Epoch 147/400\n",
      "200/200 [==============================] - 0s 23us/step - loss: 0.4689 - val_loss: 0.0646\n",
      "Epoch 148/400\n",
      "200/200 [==============================] - 0s 24us/step - loss: 0.5249 - val_loss: 0.0647\n",
      "Epoch 149/400\n",
      "200/200 [==============================] - 0s 23us/step - loss: 0.7875 - val_loss: 0.0647\n",
      "Epoch 150/400\n",
      "200/200 [==============================] - 0s 27us/step - loss: 0.2946 - val_loss: 0.0647\n",
      "Epoch 151/400\n",
      "200/200 [==============================] - 0s 28us/step - loss: 0.5906 - val_loss: 0.0646\n",
      "Epoch 152/400\n",
      "200/200 [==============================] - 0s 18us/step - loss: 0.2699 - val_loss: 0.0645\n",
      "Epoch 153/400\n",
      "200/200 [==============================] - 0s 23us/step - loss: 0.4708 - val_loss: 0.0645\n",
      "Epoch 154/400\n",
      "200/200 [==============================] - 0s 24us/step - loss: 0.3698 - val_loss: 0.0645\n",
      "Epoch 155/400\n",
      "200/200 [==============================] - 0s 23us/step - loss: 0.3222 - val_loss: 0.0644\n",
      "Epoch 156/400\n",
      "200/200 [==============================] - 0s 23us/step - loss: 0.4799 - val_loss: 0.0643\n",
      "Epoch 157/400\n",
      "200/200 [==============================] - 0s 26us/step - loss: 0.2792 - val_loss: 0.0643\n",
      "Epoch 158/400\n",
      "200/200 [==============================] - 0s 26us/step - loss: 0.4411 - val_loss: 0.0644\n",
      "Epoch 159/400\n",
      "200/200 [==============================] - 0s 28us/step - loss: 0.4297 - val_loss: 0.0643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 160/400\n",
      "200/200 [==============================] - 0s 24us/step - loss: 0.3880 - val_loss: 0.0642\n",
      "Epoch 161/400\n",
      "200/200 [==============================] - 0s 26us/step - loss: 0.2819 - val_loss: 0.0642\n",
      "Epoch 162/400\n",
      "200/200 [==============================] - 0s 24us/step - loss: 0.3804 - val_loss: 0.0642\n",
      "Epoch 163/400\n",
      "200/200 [==============================] - 0s 25us/step - loss: 0.2262 - val_loss: 0.0641\n",
      "Epoch 164/400\n",
      "200/200 [==============================] - 0s 26us/step - loss: 0.5210 - val_loss: 0.0642\n",
      "Epoch 165/400\n",
      "200/200 [==============================] - 0s 23us/step - loss: 0.2782 - val_loss: 0.0641\n",
      "Epoch 166/400\n",
      "200/200 [==============================] - 0s 24us/step - loss: 0.4771 - val_loss: 0.0640\n",
      "Epoch 167/400\n",
      "200/200 [==============================] - 0s 25us/step - loss: 0.5836 - val_loss: 0.0640\n",
      "Epoch 168/400\n",
      "200/200 [==============================] - 0s 21us/step - loss: 0.3172 - val_loss: 0.0640\n",
      "Epoch 169/400\n",
      "200/200 [==============================] - 0s 25us/step - loss: 0.3964 - val_loss: 0.0640\n",
      "Epoch 170/400\n",
      "200/200 [==============================] - 0s 25us/step - loss: 0.4248 - val_loss: 0.0638\n",
      "Epoch 171/400\n",
      "200/200 [==============================] - 0s 23us/step - loss: 0.3452 - val_loss: 0.0637\n",
      "Epoch 172/400\n",
      "200/200 [==============================] - 0s 27us/step - loss: 0.3882 - val_loss: 0.0638\n",
      "Epoch 173/400\n",
      "200/200 [==============================] - 0s 21us/step - loss: 0.3273 - val_loss: 0.0637\n",
      "Epoch 174/400\n",
      "200/200 [==============================] - 0s 24us/step - loss: 0.5497 - val_loss: 0.0638\n",
      "Epoch 175/400\n",
      "200/200 [==============================] - 0s 28us/step - loss: 0.3822 - val_loss: 0.0638\n",
      "Epoch 176/400\n",
      "200/200 [==============================] - 0s 22us/step - loss: 0.5978 - val_loss: 0.0636\n",
      "Epoch 177/400\n",
      "200/200 [==============================] - 0s 30us/step - loss: 0.4727 - val_loss: 0.0636\n",
      "Epoch 178/400\n",
      "200/200 [==============================] - 0s 24us/step - loss: 0.3665 - val_loss: 0.0636\n",
      "Epoch 179/400\n",
      "200/200 [==============================] - 0s 27us/step - loss: 0.3963 - val_loss: 0.0635\n",
      "Epoch 180/400\n",
      "200/200 [==============================] - 0s 27us/step - loss: 0.2624 - val_loss: 0.0635\n",
      "Epoch 181/400\n",
      "200/200 [==============================] - 0s 23us/step - loss: 0.5187 - val_loss: 0.0635\n",
      "Epoch 182/400\n",
      "200/200 [==============================] - 0s 23us/step - loss: 0.5408 - val_loss: 0.0635\n",
      "Epoch 183/400\n",
      "200/200 [==============================] - 0s 27us/step - loss: 0.4463 - val_loss: 0.0634\n",
      "Epoch 184/400\n",
      "200/200 [==============================] - 0s 26us/step - loss: 0.3274 - val_loss: 0.0634\n",
      "Epoch 185/400\n",
      "200/200 [==============================] - 0s 26us/step - loss: 0.2748 - val_loss: 0.0633\n",
      "Epoch 186/400\n",
      "200/200 [==============================] - 0s 25us/step - loss: 0.3951 - val_loss: 0.0634\n",
      "Epoch 187/400\n",
      "200/200 [==============================] - 0s 25us/step - loss: 0.2773 - val_loss: 0.0633\n",
      "Epoch 188/400\n",
      "200/200 [==============================] - 0s 25us/step - loss: 0.6226 - val_loss: 0.0633\n",
      "Epoch 189/400\n",
      "200/200 [==============================] - 0s 34us/step - loss: 0.5630 - val_loss: 0.0633\n",
      "Epoch 190/400\n",
      "200/200 [==============================] - 0s 25us/step - loss: 0.3105 - val_loss: 0.0632\n",
      "Epoch 191/400\n",
      "200/200 [==============================] - 0s 24us/step - loss: 0.3460 - val_loss: 0.0632\n",
      "Epoch 192/400\n",
      "200/200 [==============================] - 0s 27us/step - loss: 0.3205 - val_loss: 0.0632\n",
      "Epoch 193/400\n",
      "200/200 [==============================] - 0s 21us/step - loss: 0.3499 - val_loss: 0.0632\n",
      "Epoch 194/400\n",
      "200/200 [==============================] - 0s 23us/step - loss: 0.2665 - val_loss: 0.0631\n",
      "Epoch 195/400\n",
      "200/200 [==============================] - 0s 23us/step - loss: 0.3737 - val_loss: 0.0631\n",
      "Epoch 196/400\n",
      "200/200 [==============================] - 0s 19us/step - loss: 0.4279 - val_loss: 0.0629\n",
      "Epoch 197/400\n",
      "200/200 [==============================] - 0s 23us/step - loss: 0.3652 - val_loss: 0.0629\n",
      "Epoch 198/400\n",
      "200/200 [==============================] - 0s 25us/step - loss: 0.5352 - val_loss: 0.0630\n",
      "Epoch 199/400\n",
      "200/200 [==============================] - 0s 19us/step - loss: 0.5094 - val_loss: 0.0628\n",
      "Epoch 200/400\n",
      "200/200 [==============================] - 0s 20us/step - loss: 0.3934 - val_loss: 0.0629\n",
      "Epoch 201/400\n",
      "200/200 [==============================] - 0s 19us/step - loss: 0.3558 - val_loss: 0.0628\n",
      "Epoch 202/400\n",
      "200/200 [==============================] - 0s 20us/step - loss: 0.2966 - val_loss: 0.0628\n",
      "Epoch 203/400\n",
      "200/200 [==============================] - 0s 23us/step - loss: 0.2118 - val_loss: 0.0628\n",
      "Epoch 204/400\n",
      "200/200 [==============================] - 0s 24us/step - loss: 0.3283 - val_loss: 0.0629\n",
      "Epoch 205/400\n",
      "200/200 [==============================] - 0s 24us/step - loss: 0.3766 - val_loss: 0.0629\n",
      "Epoch 206/400\n",
      "200/200 [==============================] - 0s 28us/step - loss: 0.5348 - val_loss: 0.0628\n",
      "Epoch 207/400\n",
      "200/200 [==============================] - 0s 21us/step - loss: 0.6281 - val_loss: 0.0628\n",
      "Epoch 208/400\n",
      "200/200 [==============================] - 0s 24us/step - loss: 0.3505 - val_loss: 0.0628\n",
      "Epoch 209/400\n",
      "200/200 [==============================] - 0s 24us/step - loss: 0.3246 - val_loss: 0.0628\n",
      "Epoch 210/400\n",
      "200/200 [==============================] - 0s 23us/step - loss: 0.3605 - val_loss: 0.0627\n",
      "Epoch 211/400\n",
      "200/200 [==============================] - 0s 25us/step - loss: 0.2424 - val_loss: 0.0627\n",
      "Epoch 212/400\n",
      "200/200 [==============================] - 0s 25us/step - loss: 0.2978 - val_loss: 0.0627\n",
      "Epoch 213/400\n",
      "200/200 [==============================] - 0s 28us/step - loss: 0.4075 - val_loss: 0.0627\n",
      "Epoch 214/400\n",
      "200/200 [==============================] - 0s 32us/step - loss: 0.8820 - val_loss: 0.0627\n",
      "Epoch 215/400\n",
      "200/200 [==============================] - 0s 25us/step - loss: 0.3072 - val_loss: 0.0626\n",
      "Epoch 216/400\n",
      "200/200 [==============================] - 0s 22us/step - loss: 0.3342 - val_loss: 0.0626\n",
      "Epoch 217/400\n",
      "200/200 [==============================] - 0s 26us/step - loss: 0.3315 - val_loss: 0.0626\n",
      "Epoch 218/400\n",
      "200/200 [==============================] - 0s 26us/step - loss: 0.3954 - val_loss: 0.0625\n",
      "Epoch 219/400\n",
      "200/200 [==============================] - 0s 27us/step - loss: 0.1750 - val_loss: 0.0625\n",
      "Epoch 220/400\n",
      "200/200 [==============================] - 0s 23us/step - loss: 0.3596 - val_loss: 0.0624\n",
      "Epoch 221/400\n",
      "200/200 [==============================] - 0s 25us/step - loss: 0.1986 - val_loss: 0.0624\n",
      "Epoch 222/400\n",
      "200/200 [==============================] - 0s 23us/step - loss: 0.2976 - val_loss: 0.0624\n",
      "Epoch 223/400\n",
      "200/200 [==============================] - 0s 24us/step - loss: 0.2321 - val_loss: 0.0624\n",
      "Epoch 224/400\n",
      "200/200 [==============================] - 0s 24us/step - loss: 0.2748 - val_loss: 0.0624\n",
      "Epoch 225/400\n",
      "200/200 [==============================] - 0s 26us/step - loss: 0.2808 - val_loss: 0.0624\n",
      "Epoch 226/400\n",
      "200/200 [==============================] - 0s 24us/step - loss: 0.3483 - val_loss: 0.0623\n",
      "Epoch 227/400\n",
      "200/200 [==============================] - 0s 29us/step - loss: 0.2460 - val_loss: 0.0623\n",
      "Epoch 228/400\n",
      "200/200 [==============================] - 0s 28us/step - loss: 0.4328 - val_loss: 0.0623\n",
      "Epoch 229/400\n",
      "200/200 [==============================] - 0s 30us/step - loss: 0.2057 - val_loss: 0.0624\n",
      "Epoch 230/400\n",
      "200/200 [==============================] - 0s 23us/step - loss: 0.3700 - val_loss: 0.0625\n",
      "Epoch 231/400\n",
      "200/200 [==============================] - 0s 21us/step - loss: 0.3638 - val_loss: 0.0625\n",
      "Epoch 232/400\n",
      "200/200 [==============================] - 0s 23us/step - loss: 0.2602 - val_loss: 0.0624\n",
      "Epoch 233/400\n",
      "200/200 [==============================] - 0s 24us/step - loss: 0.4848 - val_loss: 0.0624\n",
      "Epoch 234/400\n",
      "200/200 [==============================] - 0s 27us/step - loss: 0.3319 - val_loss: 0.0624\n",
      "Epoch 235/400\n",
      "200/200 [==============================] - 0s 27us/step - loss: 0.3664 - val_loss: 0.0624\n",
      "Epoch 236/400\n",
      "200/200 [==============================] - 0s 35us/step - loss: 0.5266 - val_loss: 0.0623\n",
      "Epoch 237/400\n",
      "200/200 [==============================] - 0s 27us/step - loss: 0.4258 - val_loss: 0.0622\n",
      "Epoch 238/400\n",
      "200/200 [==============================] - 0s 33us/step - loss: 0.2584 - val_loss: 0.0622\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 239/400\n",
      "200/200 [==============================] - 0s 24us/step - loss: 0.2571 - val_loss: 0.0622\n",
      "Epoch 240/400\n",
      "200/200 [==============================] - 0s 26us/step - loss: 0.5115 - val_loss: 0.0622\n",
      "Epoch 241/400\n",
      "200/200 [==============================] - 0s 27us/step - loss: 0.2934 - val_loss: 0.0622\n",
      "Epoch 242/400\n",
      "200/200 [==============================] - 0s 32us/step - loss: 0.2643 - val_loss: 0.0624\n",
      "Epoch 243/400\n",
      "200/200 [==============================] - 0s 23us/step - loss: 0.4046 - val_loss: 0.0623\n",
      "Epoch 244/400\n",
      "200/200 [==============================] - 0s 20us/step - loss: 0.4573 - val_loss: 0.0622\n",
      "Epoch 245/400\n",
      "200/200 [==============================] - 0s 26us/step - loss: 0.4040 - val_loss: 0.0623\n",
      "Epoch 246/400\n",
      "200/200 [==============================] - 0s 29us/step - loss: 0.7285 - val_loss: 0.0621\n",
      "Epoch 247/400\n",
      "200/200 [==============================] - 0s 26us/step - loss: 0.2733 - val_loss: 0.0620\n",
      "Epoch 248/400\n",
      "200/200 [==============================] - 0s 26us/step - loss: 0.3555 - val_loss: 0.0620\n",
      "Epoch 249/400\n",
      "200/200 [==============================] - 0s 21us/step - loss: 0.4371 - val_loss: 0.0619\n",
      "Epoch 250/400\n",
      "200/200 [==============================] - 0s 26us/step - loss: 0.3592 - val_loss: 0.0617\n",
      "Epoch 251/400\n",
      "200/200 [==============================] - 0s 21us/step - loss: 0.3911 - val_loss: 0.0617\n",
      "Epoch 252/400\n",
      "200/200 [==============================] - 0s 27us/step - loss: 0.6056 - val_loss: 0.0617\n",
      "Epoch 253/400\n",
      "200/200 [==============================] - 0s 26us/step - loss: 0.2817 - val_loss: 0.0617\n",
      "Epoch 254/400\n",
      "200/200 [==============================] - 0s 22us/step - loss: 0.4199 - val_loss: 0.0617\n",
      "Epoch 255/400\n",
      "200/200 [==============================] - 0s 24us/step - loss: 0.3646 - val_loss: 0.0618\n",
      "Epoch 256/400\n",
      "200/200 [==============================] - 0s 26us/step - loss: 0.2498 - val_loss: 0.0618\n",
      "Epoch 257/400\n",
      "200/200 [==============================] - 0s 22us/step - loss: 0.3064 - val_loss: 0.0619\n",
      "Epoch 258/400\n",
      "200/200 [==============================] - 0s 23us/step - loss: 0.3508 - val_loss: 0.0619\n",
      "Epoch 259/400\n",
      "200/200 [==============================] - 0s 19us/step - loss: 0.2778 - val_loss: 0.0619\n",
      "Epoch 260/400\n",
      "200/200 [==============================] - 0s 22us/step - loss: 0.4375 - val_loss: 0.0620\n",
      "Epoch 261/400\n",
      "200/200 [==============================] - 0s 26us/step - loss: 0.2589 - val_loss: 0.0620\n",
      "Epoch 262/400\n",
      "200/200 [==============================] - 0s 19us/step - loss: 0.3233 - val_loss: 0.0620\n",
      "Epoch 263/400\n",
      "200/200 [==============================] - 0s 21us/step - loss: 0.4204 - val_loss: 0.0620\n",
      "Epoch 264/400\n",
      "200/200 [==============================] - 0s 27us/step - loss: 0.4764 - val_loss: 0.0621\n",
      "Epoch 265/400\n",
      "200/200 [==============================] - 0s 22us/step - loss: 0.2890 - val_loss: 0.0620\n",
      "Epoch 266/400\n",
      "200/200 [==============================] - 0s 24us/step - loss: 0.3970 - val_loss: 0.0621\n",
      "Epoch 267/400\n",
      "200/200 [==============================] - 0s 22us/step - loss: 0.3935 - val_loss: 0.0623\n",
      "Epoch 268/400\n",
      "200/200 [==============================] - 0s 20us/step - loss: 0.2667 - val_loss: 0.0623\n",
      "Epoch 269/400\n",
      "200/200 [==============================] - 0s 24us/step - loss: 0.2246 - val_loss: 0.0623\n",
      "Epoch 270/400\n",
      "200/200 [==============================] - 0s 22us/step - loss: 0.2785 - val_loss: 0.0621\n",
      "Epoch 271/400\n",
      "200/200 [==============================] - 0s 19us/step - loss: 0.4155 - val_loss: 0.0620\n",
      "Epoch 272/400\n",
      "200/200 [==============================] - 0s 21us/step - loss: 0.2629 - val_loss: 0.0619\n",
      "Epoch 273/400\n",
      "200/200 [==============================] - 0s 24us/step - loss: 0.6754 - val_loss: 0.0620\n",
      "Epoch 274/400\n",
      "200/200 [==============================] - 0s 24us/step - loss: 0.2887 - val_loss: 0.0618\n",
      "Epoch 275/400\n",
      "200/200 [==============================] - 0s 26us/step - loss: 0.2860 - val_loss: 0.0618\n",
      "Epoch 276/400\n",
      "200/200 [==============================] - 0s 22us/step - loss: 0.3786 - val_loss: 0.0618\n",
      "Epoch 277/400\n",
      "200/200 [==============================] - 0s 23us/step - loss: 0.2608 - val_loss: 0.0618\n",
      "Epoch 278/400\n",
      "200/200 [==============================] - 0s 27us/step - loss: 0.5014 - val_loss: 0.0618\n",
      "Epoch 279/400\n",
      "200/200 [==============================] - 0s 17us/step - loss: 0.2256 - val_loss: 0.0617\n",
      "Epoch 280/400\n",
      "200/200 [==============================] - 0s 26us/step - loss: 0.4816 - val_loss: 0.0617\n",
      "Epoch 281/400\n",
      "200/200 [==============================] - 0s 26us/step - loss: 0.3621 - val_loss: 0.0617\n",
      "Epoch 282/400\n",
      "200/200 [==============================] - 0s 20us/step - loss: 0.2895 - val_loss: 0.0618\n",
      "Epoch 283/400\n",
      "200/200 [==============================] - 0s 23us/step - loss: 0.3048 - val_loss: 0.0618\n",
      "Epoch 284/400\n",
      "200/200 [==============================] - 0s 27us/step - loss: 0.2942 - val_loss: 0.0617\n",
      "Epoch 285/400\n",
      "200/200 [==============================] - 0s 23us/step - loss: 0.2132 - val_loss: 0.0617\n",
      "Epoch 286/400\n",
      "200/200 [==============================] - 0s 23us/step - loss: 0.3621 - val_loss: 0.0616\n",
      "Epoch 287/400\n",
      "200/200 [==============================] - 0s 22us/step - loss: 0.3997 - val_loss: 0.0615\n",
      "Epoch 288/400\n",
      "200/200 [==============================] - 0s 25us/step - loss: 0.2752 - val_loss: 0.0615\n",
      "Epoch 289/400\n",
      "200/200 [==============================] - 0s 26us/step - loss: 0.2598 - val_loss: 0.0615\n",
      "Epoch 290/400\n",
      "200/200 [==============================] - 0s 20us/step - loss: 0.6416 - val_loss: 0.0614\n",
      "Epoch 291/400\n",
      "200/200 [==============================] - 0s 23us/step - loss: 0.2983 - val_loss: 0.0614\n",
      "Epoch 292/400\n",
      "200/200 [==============================] - 0s 26us/step - loss: 0.5065 - val_loss: 0.0613\n",
      "Epoch 293/400\n",
      "200/200 [==============================] - 0s 21us/step - loss: 0.2840 - val_loss: 0.0612\n",
      "Epoch 294/400\n",
      "200/200 [==============================] - 0s 24us/step - loss: 0.3356 - val_loss: 0.0613\n",
      "Epoch 295/400\n",
      "200/200 [==============================] - 0s 27us/step - loss: 0.5042 - val_loss: 0.0611\n",
      "Epoch 296/400\n",
      "200/200 [==============================] - 0s 22us/step - loss: 0.3836 - val_loss: 0.0611\n",
      "Epoch 297/400\n",
      "200/200 [==============================] - 0s 24us/step - loss: 0.2974 - val_loss: 0.0611\n",
      "Epoch 298/400\n",
      "200/200 [==============================] - 0s 22us/step - loss: 0.2528 - val_loss: 0.0611\n",
      "Epoch 299/400\n",
      "200/200 [==============================] - 0s 23us/step - loss: 0.3228 - val_loss: 0.0611\n",
      "Epoch 300/400\n",
      "200/200 [==============================] - 0s 25us/step - loss: 0.2386 - val_loss: 0.0612\n",
      "Epoch 301/400\n",
      "200/200 [==============================] - 0s 21us/step - loss: 0.2139 - val_loss: 0.0611\n",
      "Epoch 302/400\n",
      "200/200 [==============================] - 0s 24us/step - loss: 0.4185 - val_loss: 0.0610\n",
      "Epoch 303/400\n",
      "200/200 [==============================] - 0s 26us/step - loss: 0.4008 - val_loss: 0.0612\n",
      "Epoch 304/400\n",
      "200/200 [==============================] - 0s 22us/step - loss: 0.2708 - val_loss: 0.0612\n",
      "Epoch 305/400\n",
      "200/200 [==============================] - 0s 25us/step - loss: 0.4098 - val_loss: 0.0611\n",
      "Epoch 306/400\n",
      "200/200 [==============================] - 0s 25us/step - loss: 0.4117 - val_loss: 0.0610\n",
      "Epoch 307/400\n",
      "200/200 [==============================] - 0s 26us/step - loss: 0.2842 - val_loss: 0.0609\n",
      "Epoch 308/400\n",
      "200/200 [==============================] - 0s 24us/step - loss: 0.3710 - val_loss: 0.0608\n",
      "Epoch 309/400\n",
      "200/200 [==============================] - 0s 21us/step - loss: 0.3710 - val_loss: 0.0607\n",
      "Epoch 310/400\n",
      "200/200 [==============================] - 0s 21us/step - loss: 0.2546 - val_loss: 0.0607\n",
      "Epoch 311/400\n",
      "200/200 [==============================] - 0s 22us/step - loss: 0.2325 - val_loss: 0.0607\n",
      "Epoch 312/400\n",
      "200/200 [==============================] - 0s 22us/step - loss: 0.3234 - val_loss: 0.0606\n",
      "Epoch 313/400\n",
      "200/200 [==============================] - 0s 21us/step - loss: 0.5904 - val_loss: 0.0606\n",
      "Epoch 314/400\n",
      "200/200 [==============================] - 0s 26us/step - loss: 0.3196 - val_loss: 0.0605\n",
      "Epoch 315/400\n",
      "200/200 [==============================] - 0s 23us/step - loss: 0.3348 - val_loss: 0.0605\n",
      "Epoch 316/400\n",
      "200/200 [==============================] - 0s 20us/step - loss: 0.3343 - val_loss: 0.0605\n",
      "Epoch 317/400\n",
      "200/200 [==============================] - 0s 24us/step - loss: 0.4481 - val_loss: 0.0604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 318/400\n",
      "200/200 [==============================] - 0s 21us/step - loss: 0.2721 - val_loss: 0.0605\n",
      "Epoch 319/400\n",
      "200/200 [==============================] - 0s 21us/step - loss: 0.2161 - val_loss: 0.0605\n",
      "Epoch 320/400\n",
      "200/200 [==============================] - 0s 21us/step - loss: 0.2528 - val_loss: 0.0605\n",
      "Epoch 321/400\n",
      "200/200 [==============================] - 0s 23us/step - loss: 0.2329 - val_loss: 0.0604\n",
      "Epoch 322/400\n",
      "200/200 [==============================] - 0s 22us/step - loss: 0.3057 - val_loss: 0.0604\n",
      "Epoch 323/400\n",
      "200/200 [==============================] - 0s 21us/step - loss: 0.4000 - val_loss: 0.0604\n",
      "Epoch 324/400\n",
      "200/200 [==============================] - 0s 28us/step - loss: 0.2889 - val_loss: 0.0602\n",
      "Epoch 325/400\n",
      "200/200 [==============================] - 0s 22us/step - loss: 0.4104 - val_loss: 0.0601\n",
      "Epoch 326/400\n",
      "200/200 [==============================] - 0s 24us/step - loss: 0.2344 - val_loss: 0.0600\n",
      "Epoch 327/400\n",
      "200/200 [==============================] - 0s 25us/step - loss: 0.4914 - val_loss: 0.0599\n",
      "Epoch 328/400\n",
      "200/200 [==============================] - 0s 22us/step - loss: 0.2406 - val_loss: 0.0599\n",
      "Epoch 329/400\n",
      "200/200 [==============================] - 0s 29us/step - loss: 0.2973 - val_loss: 0.0599\n",
      "Epoch 330/400\n",
      "200/200 [==============================] - 0s 24us/step - loss: 0.2748 - val_loss: 0.0599\n",
      "Epoch 331/400\n",
      "200/200 [==============================] - 0s 25us/step - loss: 0.3497 - val_loss: 0.0599\n",
      "Epoch 332/400\n",
      "200/200 [==============================] - 0s 32us/step - loss: 0.2452 - val_loss: 0.0599\n",
      "Epoch 333/400\n",
      "200/200 [==============================] - 0s 25us/step - loss: 0.2404 - val_loss: 0.0599\n",
      "Epoch 334/400\n",
      "200/200 [==============================] - 0s 25us/step - loss: 0.2713 - val_loss: 0.0599\n",
      "Epoch 335/400\n",
      "200/200 [==============================] - 0s 21us/step - loss: 0.2408 - val_loss: 0.0598\n",
      "Epoch 336/400\n",
      "200/200 [==============================] - 0s 24us/step - loss: 0.2792 - val_loss: 0.0598\n",
      "Epoch 337/400\n",
      "200/200 [==============================] - 0s 24us/step - loss: 0.2675 - val_loss: 0.0599\n",
      "Epoch 338/400\n",
      "200/200 [==============================] - 0s 19us/step - loss: 0.3220 - val_loss: 0.0598\n",
      "Epoch 339/400\n",
      "200/200 [==============================] - 0s 23us/step - loss: 0.3696 - val_loss: 0.0597\n",
      "Epoch 340/400\n",
      "200/200 [==============================] - 0s 27us/step - loss: 0.2665 - val_loss: 0.0597\n",
      "Epoch 341/400\n",
      "200/200 [==============================] - 0s 23us/step - loss: 0.2737 - val_loss: 0.0597\n",
      "Epoch 342/400\n",
      "200/200 [==============================] - 0s 25us/step - loss: 0.2884 - val_loss: 0.0597\n",
      "Epoch 343/400\n",
      "200/200 [==============================] - 0s 22us/step - loss: 0.4430 - val_loss: 0.0598\n",
      "Epoch 344/400\n",
      "200/200 [==============================] - 0s 23us/step - loss: 0.3281 - val_loss: 0.0597\n",
      "Epoch 345/400\n",
      "200/200 [==============================] - 0s 26us/step - loss: 0.3367 - val_loss: 0.0597\n",
      "Epoch 346/400\n",
      "200/200 [==============================] - 0s 21us/step - loss: 0.2344 - val_loss: 0.0596\n",
      "Epoch 347/400\n",
      "200/200 [==============================] - 0s 25us/step - loss: 0.4077 - val_loss: 0.0597\n",
      "Epoch 348/400\n",
      "200/200 [==============================] - 0s 25us/step - loss: 0.3991 - val_loss: 0.0596\n",
      "Epoch 349/400\n",
      "200/200 [==============================] - 0s 21us/step - loss: 0.3359 - val_loss: 0.0597\n",
      "Epoch 350/400\n",
      "200/200 [==============================] - 0s 23us/step - loss: 0.2176 - val_loss: 0.0596\n",
      "Epoch 351/400\n",
      "200/200 [==============================] - 0s 27us/step - loss: 0.2709 - val_loss: 0.0596\n",
      "Epoch 352/400\n",
      "200/200 [==============================] - 0s 26us/step - loss: 0.2178 - val_loss: 0.0595\n",
      "Epoch 353/400\n",
      "200/200 [==============================] - 0s 29us/step - loss: 0.3153 - val_loss: 0.0595\n",
      "Epoch 354/400\n",
      "200/200 [==============================] - 0s 23us/step - loss: 0.2225 - val_loss: 0.0595\n",
      "Epoch 355/400\n",
      "200/200 [==============================] - 0s 24us/step - loss: 0.4124 - val_loss: 0.0596\n",
      "Epoch 356/400\n",
      "200/200 [==============================] - 0s 25us/step - loss: 0.2626 - val_loss: 0.0595\n",
      "Epoch 357/400\n",
      "200/200 [==============================] - 0s 25us/step - loss: 0.3341 - val_loss: 0.0594\n",
      "Epoch 358/400\n",
      "200/200 [==============================] - 0s 27us/step - loss: 0.3054 - val_loss: 0.0594\n",
      "Epoch 359/400\n",
      "200/200 [==============================] - 0s 24us/step - loss: 0.3124 - val_loss: 0.0593\n",
      "Epoch 360/400\n",
      "200/200 [==============================] - 0s 21us/step - loss: 0.1975 - val_loss: 0.0593\n",
      "Epoch 361/400\n",
      "200/200 [==============================] - 0s 23us/step - loss: 0.3163 - val_loss: 0.0593\n",
      "Epoch 362/400\n",
      "200/200 [==============================] - 0s 26us/step - loss: 0.3170 - val_loss: 0.0593\n",
      "Epoch 363/400\n",
      "200/200 [==============================] - 0s 23us/step - loss: 0.2899 - val_loss: 0.0592\n",
      "Epoch 364/400\n",
      "200/200 [==============================] - 0s 28us/step - loss: 0.1704 - val_loss: 0.0593\n",
      "Epoch 365/400\n",
      "200/200 [==============================] - 0s 22us/step - loss: 0.2255 - val_loss: 0.0593\n",
      "Epoch 366/400\n",
      "200/200 [==============================] - 0s 28us/step - loss: 0.2130 - val_loss: 0.0593\n",
      "Epoch 367/400\n",
      "200/200 [==============================] - 0s 25us/step - loss: 0.4050 - val_loss: 0.0592\n",
      "Epoch 368/400\n",
      "200/200 [==============================] - 0s 26us/step - loss: 0.3343 - val_loss: 0.0592\n",
      "Epoch 369/400\n",
      "200/200 [==============================] - 0s 26us/step - loss: 0.2652 - val_loss: 0.0592\n",
      "Epoch 370/400\n",
      "200/200 [==============================] - 0s 23us/step - loss: 0.2345 - val_loss: 0.0591\n",
      "Epoch 371/400\n",
      "200/200 [==============================] - 0s 23us/step - loss: 0.2923 - val_loss: 0.0594\n",
      "Epoch 372/400\n",
      "200/200 [==============================] - 0s 23us/step - loss: 0.6151 - val_loss: 0.0594\n",
      "Epoch 373/400\n",
      "200/200 [==============================] - 0s 23us/step - loss: 0.3051 - val_loss: 0.0594\n",
      "Epoch 374/400\n",
      "200/200 [==============================] - 0s 28us/step - loss: 0.2615 - val_loss: 0.0594\n",
      "Epoch 375/400\n",
      "200/200 [==============================] - 0s 23us/step - loss: 0.2785 - val_loss: 0.0594\n",
      "Epoch 376/400\n",
      "200/200 [==============================] - 0s 24us/step - loss: 0.3144 - val_loss: 0.0593\n",
      "Epoch 377/400\n",
      "200/200 [==============================] - 0s 24us/step - loss: 0.2408 - val_loss: 0.0593\n",
      "Epoch 378/400\n",
      "200/200 [==============================] - 0s 20us/step - loss: 0.2470 - val_loss: 0.0593\n",
      "Epoch 379/400\n",
      "200/200 [==============================] - 0s 24us/step - loss: 0.3321 - val_loss: 0.0593\n",
      "Epoch 380/400\n",
      "200/200 [==============================] - 0s 22us/step - loss: 0.5581 - val_loss: 0.0593\n",
      "Epoch 381/400\n",
      "200/200 [==============================] - 0s 19us/step - loss: 0.1807 - val_loss: 0.0594\n",
      "Epoch 382/400\n",
      "200/200 [==============================] - 0s 25us/step - loss: 0.2503 - val_loss: 0.0593\n",
      "Epoch 383/400\n",
      "200/200 [==============================] - 0s 22us/step - loss: 0.2573 - val_loss: 0.0592\n",
      "Epoch 384/400\n",
      "200/200 [==============================] - 0s 21us/step - loss: 0.2476 - val_loss: 0.0592\n",
      "Epoch 385/400\n",
      "200/200 [==============================] - 0s 29us/step - loss: 0.3404 - val_loss: 0.0591\n",
      "Epoch 386/400\n",
      "200/200 [==============================] - 0s 28us/step - loss: 0.5130 - val_loss: 0.0591\n",
      "Epoch 387/400\n",
      "200/200 [==============================] - 0s 30us/step - loss: 0.2897 - val_loss: 0.0592\n",
      "Epoch 388/400\n",
      "200/200 [==============================] - 0s 26us/step - loss: 0.1765 - val_loss: 0.0591\n",
      "Epoch 389/400\n",
      "200/200 [==============================] - 0s 23us/step - loss: 0.3578 - val_loss: 0.0591\n",
      "Epoch 390/400\n",
      "200/200 [==============================] - 0s 23us/step - loss: 0.1910 - val_loss: 0.0590\n",
      "Epoch 391/400\n",
      "200/200 [==============================] - 0s 23us/step - loss: 0.2177 - val_loss: 0.0589\n",
      "Epoch 392/400\n",
      "200/200 [==============================] - 0s 20us/step - loss: 0.2712 - val_loss: 0.0590\n",
      "Epoch 393/400\n",
      "200/200 [==============================] - 0s 28us/step - loss: 0.2030 - val_loss: 0.0588\n",
      "Epoch 394/400\n",
      "200/200 [==============================] - 0s 21us/step - loss: 0.3200 - val_loss: 0.0589\n",
      "Epoch 395/400\n",
      "200/200 [==============================] - 0s 24us/step - loss: 0.5834 - val_loss: 0.0588\n",
      "Epoch 396/400\n",
      "200/200 [==============================] - 0s 23us/step - loss: 0.2960 - val_loss: 0.0589\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 397/400\n",
      "200/200 [==============================] - 0s 20us/step - loss: 0.2350 - val_loss: 0.0589\n",
      "Epoch 398/400\n",
      "200/200 [==============================] - 0s 25us/step - loss: 0.2820 - val_loss: 0.0588\n",
      "Epoch 399/400\n",
      "200/200 [==============================] - 0s 24us/step - loss: 0.2350 - val_loss: 0.0587\n",
      "Epoch 400/400\n",
      "200/200 [==============================] - 0s 28us/step - loss: 0.2562 - val_loss: 0.0587\n",
      "Concordance Index for training dataset: 0.4547980981718342\n",
      "Concordance Index for test dataset: 0.43823915900131405\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <td>lifelines.CoxPHFitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>duration col</th>\n",
       "      <td>'week'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>event col</th>\n",
       "      <td>'arrest'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>baseline estimation</th>\n",
       "      <td>breslow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>number of observations</th>\n",
       "      <td>432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>number of events observed</th>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>partial log-likelihood</th>\n",
       "      <td>-658.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time fit was run</th>\n",
       "      <td>2020-04-06 11:41:40 UTC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>exp(coef)</th>\n",
       "      <th>se(coef)</th>\n",
       "      <th>coef lower 95%</th>\n",
       "      <th>coef upper 95%</th>\n",
       "      <th>exp(coef) lower 95%</th>\n",
       "      <th>exp(coef) upper 95%</th>\n",
       "      <th>z</th>\n",
       "      <th>p</th>\n",
       "      <th>-log2(p)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fin</th>\n",
       "      <td>-0.38</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.19</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.47</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-1.98</td>\n",
       "      <td>0.05</td>\n",
       "      <td>4.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>-0.06</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.99</td>\n",
       "      <td>-2.61</td>\n",
       "      <td>0.01</td>\n",
       "      <td>6.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>race</th>\n",
       "      <td>0.31</td>\n",
       "      <td>1.37</td>\n",
       "      <td>0.31</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.75</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1.02</td>\n",
       "      <td>0.31</td>\n",
       "      <td>1.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wexp</th>\n",
       "      <td>-0.15</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.21</td>\n",
       "      <td>-0.57</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.57</td>\n",
       "      <td>1.30</td>\n",
       "      <td>-0.71</td>\n",
       "      <td>0.48</td>\n",
       "      <td>1.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mar</th>\n",
       "      <td>-0.43</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.38</td>\n",
       "      <td>-1.18</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.31</td>\n",
       "      <td>1.37</td>\n",
       "      <td>-1.14</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paro</th>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-0.47</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.63</td>\n",
       "      <td>1.35</td>\n",
       "      <td>-0.43</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prio</th>\n",
       "      <td>0.09</td>\n",
       "      <td>1.10</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.15</td>\n",
       "      <td>1.04</td>\n",
       "      <td>1.16</td>\n",
       "      <td>3.19</td>\n",
       "      <td>&lt;0.005</td>\n",
       "      <td>9.48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Log-likelihood ratio test</th>\n",
       "      <td>33.27 on 7 df, -log2(p)=15.37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Training...')\n",
    "model.fit(X_train, Y_train,validation_split=0.33, batch_size=324, nb_epoch=400, shuffle=False, callbacks=[tensoboard])\n",
    "\n",
    "hr_pred=model.predict(X_train)\n",
    "hr_pred=np.exp(hr_pred)\n",
    "ci=concordance_index(Y_train,-hr_pred,E_train)\n",
    "\n",
    "hr_pred2=model.predict(X_val)\n",
    "hr_pred2=np.exp(hr_pred2)\n",
    "ci2=concordance_index(Y_val,-hr_pred2,E_val)\n",
    "print ('Concordance Index for training dataset:', ci)\n",
    "print ('Concordance Index for test dataset:', ci2)\n",
    "\n",
    "#Cox Fitting\n",
    "cf = CoxPHFitter()\n",
    "cf.fit(rossi_dataset, 'week', event_col='arrest')\n",
    "\n",
    "cf.print_summary()  # access the results using cf.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://localhost:6009\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x14b8b6950>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
